# Performing some useful analyses in R

I'm going to give some examples of how to do common analyses you'll need for this class. I won't be spending much time on the statistical assumptions or diagnostics.

```{r tidyverse_workaround_a4, echo = FALSE, message = FALSE}
# This invisible block is a workaround for Travis CI not having the tidyverse meta-package; 
library(ggplot2)
library(dplyr)
library(readr)
library(tibble)
library(forcats)
library(cowplot)
theme_set(theme_cowplot())
lizards <- read_csv("example_data/anoles.csv") # See Appendix A if you don't have this data
# Capture View()
real_view = View
View = function(x,...){
  # Scrollable table output
  # knitr::kable(head(x))
  x %>% head(n = 10) %>% 
    kableExtra::kbl() %>% 
    kableExtra::kable_paper() %>% 
    kableExtra::scroll_box(width = "100%")
} 
```
```{r load_anoles_a4, echo = TRUE, eval=FALSE}
library(tidyverse) 
library(cowplot)
theme_set(theme_cowplot())
lizards <- read_csv("example_data/anoles.csv") # See Appendix A if you don't have this data
```

## A note on factors

R has two ways of representing textual data: character vectors (also called strings) and factors. 

 - Character vectors are just text; they have no inherent underlying meaning. 
 - Factors are a data type with a specific number of levels; they're often used to represent different experimental treatments. Examples could include {low, medium, high} or {control, treatment}. Each level of a factor is associated with a number.

For the most part, it's easier and safer to work with character vectors. Most functions we'll be using know how to convert them when it's necessary.

One important thing to note is that `ggplot` arranges character vectors alphabetically on its categorical scales, but orders factors by their level number.  Thus, to change the order of categorical x-axes (and other scales), you need to make your categories into a factor.  This is done with the `fct_inorder`, `fct_infreq`, and related functions, which are part of the `forcats` package and loaded with tidyverse. The easiest one to use is `fct_inorder`, which changes the level values to be in the order of your data; when combined with `arrange()` and other `dplyr` functions, this is quite flexible and powerful. For more information on these and other factor functions, take a look at the [forcats website](http://forcats.tidyverse.org). 

If you want to manually create a factor, you can use the `factor` command, which is in base R (no package).  

```{r manual_factor, collapse = TRUE}
color_levels = c("Red","Green","Blue")
# Randomly select 20 colors from color_levels
color_example = sample(color_levels, size = 20, replace = TRUE)
color_example

# Convert it into a factor
color_as_factor = factor(color_example, levels = color_levels)
color_as_factor
```

## Comparing categorical frequencies (Contingency Tables & related analyses)

These tests generally compare the frequencies of count data.

### Contingency Tables

The easiest way to make a contingency table is from a data frame where each column is a categorical variable you want in the table & each row is an observation.  Let's say we wanted to create a color morph by perch type contingency with our lizard data.

```{r contingency_lizard_1, collapse = TRUE}
color_by_perch_tbl = lizards %>% 
  select(Color_morph, Perch_type) %>% # select only the columns you want
  table()  # feed them into the table command
color_by_perch_tbl
```

If you want to switch your contingency table from counts to frequencies, just divide it by its sum:

```{r contingency_lizard_freq, collapse = TRUE}
color_by_perch_tbl / sum(color_by_perch_tbl)
```

### Chi-squared & Fisher's Exact Tests

Once you have a contingency table, you can test for independence between the rows and columns. This provides you with your test statistic (`X-squared`), degrees of freedom, and p-value.  

```{r chi_sq_1, collapse = TRUE}
chisq.test(color_by_perch_tbl)
```

Chi-squared tests assume that there's at least five observations in each cell of the contingency table.  If this fails, then the resulting values aren't accurate.  For example: 

```{r contingency_lizard_small, collapse = TRUE}
site_a_contingency = lizards %>% 
  filter(Site == "A") %>% # cut down the data size
  select(Color_morph, Perch_type) %>% # select only the columns you want
  table()  # feed them into the table command
print(site_a_contingency)
chisq.test(site_a_contingency)
```

Note the warning that "Chi-squared approximation may be incorrect."  In this case, it's a good idea to run the Fisher's exact test, which investigates the same null hypotheses, but works with low counts. Fisher's test is less powerful than the Chi-squared test, so it should only be used when it's the only option.

```{r exact_test_lizards, collapse = TRUE}
fisher.test(site_a_contingency)
```

You can also run a chi-squared or Fisher's exact test directly on two vectors or data frame columns:

```{r chisq_exact_direct, collapse = TRUE}
chisq.test(lizards$Color_morph, lizards$Perch_type)
```

## Linear Models: Regression and ANOVA

Linear regression and Analysis of Variance (ANOVA) are both special cases of the general linear model (LM), which fits a continuous response (y) to one or more predictors (x). You specify linear models in R with a **formula** syntax, which generally follows as: `response ~ predictor`. Combining this formula with the `lm()` function and a datset gives you the basis of a linear model.

### Regression

Lets say we wanted to see how snout-vent length (SVL) affects mass:

```{r lm_1, collapse = TRUE}
simple_reg = lm(Mass ~ SVL, data = lizards)
simple_reg
```

By default, this creates an LM object, which tells us the regression coefficients. For a linear regression (continuous response), these tell us the regression equation; in this case, that for every $1 \text{ mm}$ increase in SVL, mass increases by $`r coef(simple_reg)[2] %>% round(3) %>% as.numeric()` \text{ g}$.  Note that in this case, it may make sense to re-scale SVL to be in cm, so that the coefficient would be easier to interpret (e.g., use `lizards %>% mutate(SVL_cm = SVL/10)`. To extract the coeficients directly, use:

```{r lm_2, collapse = TRUE}
coef(simple_reg)
```

For more information, use the summary function:
```{r lm_smry, collapse = TRUE}
summary(simple_reg)
```

The most important components of this are the $R^2$ (which is listed as `Multiple R-squared`), the standard errors and p-values for each of your coefficients (`Coefficients` section), and the overall F-statistic and p-value (the last line).  

A quick note on p-values:

  1. Don't base all of your interpretation on p-values; the $R^2$ and adjusted $R^2$ of a model are more important.
  2. The overall p-value relates to how the whole model explains the variance in the data; the coefficient p-value relates to whether the specific coefficient is different from zero.  
  3. Coefficient-level p-values tend to be rather fragile, and shouldn't be used.  

#### Plotting linear regressions

For a simple linear regression, `ggplot` can automatically plot the trendline (a.k.a., fitted values) and confidence intervals with `geom_smooth()`. 

```{r a4_lin_reg_plot1, collapse = TRUE, message = FALSE}
lizards %>% 
  ggplot() + 
  aes(x = SVL, y = Mass) + 
  geom_point() + 
  geom_smooth(method = "lm", # use a linear regression
              se = TRUE, # Include a confidence interval around the line
              level = .95) # the level of the confidence interval; default = 95%

```

For more complicated models, this approach may not work very well; it can be helpful to calculate the fitted values & confidence intervals from the model object and plot them directly.  

You can get these values directly with the `predict()` function. The code below calculates the trendline and 95% confidence interval for the regression, and adds them to a data frame with the original data.  

```{r lm_predict, collapse = TRUE}
simple_reg_preds = simple_reg %>% 
  # Predict fitted values wish 95% confidence intervals
  predict(interval = "confidence", level = .95) %>% 
  as_tibble() # convert the output of predict() from a matrix to a data frame
simple_reg_plot_data = lizards %>% 
  select(SVL, Mass) %>% # we only need these columns
  bind_cols(simple_reg_preds) # adds columns of simple_reg_preds to our lizards
View(simple_reg_plot_data)
```

To plot this, you'd use a combination of `geom_line()` for the fitted values and `geom_ribbon()`, which would create the confidence interval region.  

```{r a4_lm_plot_manual}
ggplot(simple_reg_plot_data) + 
  aes(x = SVL) + 
  geom_point(aes(y = Mass), color = "cornflowerblue") + 
  geom_ribbon(aes(ymin = lwr, ymax = upr), 
              fill = alpha("black", .2),  # dark fill with 80% transparency (alpha)
              color = grey(.4), # dark-ish grey border line
              linetype = 2) +   # dotted border line
  geom_line(aes(y = fit))
```

You can also use `predict()` to calculate the respected value of your response variable given new predictor(s); this is useful for interpolation and extrapolation.  

```{r predict_lm_extrap, collapse = TRUE}
# Create a data frame with new predictors
svl_predictors = tibble(SVL = c(20, 150, 600)) # New predictors
mass_predictions = predict(simple_reg, 
                           newdata = svl_predictors)  # The newdata argument is key here
                                                      # If unspecified, it uses the original data
svl_predictors %>% 
  mutate(Mass_estimate = mass_predictions)
```

For more information, see the help page `?predict.lm`.  

### ANOVA

(In progress)

## To do (In progress)
 
### Comparing two means (t-tests)

### Advanced workflow tips

### Model comparisons
